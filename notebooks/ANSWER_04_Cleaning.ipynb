{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This document outlines the steps taken to clean and prepare the dataset for analysis. Below, each decision is explained in the context of these questions and the EDA."
      ],
      "metadata": {
        "id": "43kcmQLl1ih8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiZt85BOxwUg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset to perform data cleaning\n",
        "file_path = '/mnt/data/cleaned.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "y7UjZF_WzOjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I removed the “Unnamed: 0” column, which was an index column automatically created during file export. It does not provide any useful information for our analysis and is not necessary for answering any of our questions. Removing it also streamlined our dataset for analysis, making it easier to search for data via the “name” column."
      ],
      "metadata": {
        "id": "rpmGMbGI12zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'Unnamed: 0' column if it exists\n",
        "if 'Unnamed: 0' in data.columns:\n",
        "    data_cleaned = data.drop(columns=['Unnamed: 0'])\n",
        "else:\n",
        "    data_cleaned = data.copy()"
      ],
      "metadata": {
        "id": "TbXnXeDj1Dny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rows with all null values were removed. These rows provide no meaningful information for consumer rating or item price, and would add unnecessary size to the dataset without contributing to the analysis."
      ],
      "metadata": {
        "id": "1q795pr-3NBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with all null values\n",
        "data_cleaned = data.dropna(how='all')"
      ],
      "metadata": {
        "id": "0F3uzg2Czd75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Discount_price` and `actual_price` columns were converted from string format (with currency symbols and commas) to numeric values. The `ratings` and `no_of_ratings` columns were also cleaned and converted to numeric, removing commas and handling missing values.\n",
        "\n",
        "Numeric prices are necessary for performing operations like summing, calculating averages, and identifying the most or least purchased items based on price and discount. By converting them all to a standardised format, it makes it easier for us to conduct analyses. Similarly, numeric ratings allow us to assess the popularity of items within specific categories in a standardised manner.\n",
        "\n",
        "This helps us to perform data analysis for the following questions:\n",
        "- Fitness: Determining the best-selling products based on price and discount.\n",
        "- Travel: Identifying trends in travel-related purchases based on their price.\n",
        "- Movies/Books: Identifying the most popular films and books based on their ratings and sales.\n",
        "- Luxury/Beauty: Assessing luxury and discounted purchases using actual and discount prices.\n"
      ],
      "metadata": {
        "id": "_6bhvOd23ksX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert price columns to numeric, strip symbols and commas\n",
        "data_cleaned['discount_price'] = pd.to_numeric(data_cleaned['discount_price'].str.replace('₹', '').str.replace(',', ''), errors='coerce')\n",
        "data_cleaned['actual_price'] = pd.to_numeric(data_cleaned['actual_price'].str.replace('₹', '').str.replace(',', ''), errors='coerce')\n",
        "\n",
        "# Convert ratings and number of ratings to numeric\n",
        "data_cleaned['ratings'] = pd.to_numeric(data_cleaned['ratings'], errors='coerce')\n",
        "data_cleaned['no_of_ratings'] = pd.to_numeric(data_cleaned['no_of_ratings'].str.replace(',', ''), errors='coerce')"
      ],
      "metadata": {
        "id": "Ou7uNcahzl9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers in the `discount_price` column were removed using the IQR (Interquartile Range) method. The lower and upper bounds were calculated based on 1.5 times the IQR https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/identifying-outliers-iqr-rule.\n",
        "\n",
        "Outliers in pricing can significantly skew analysis, particularly when analysing sales trends. Removing extreme values will help us focus on the typical price range for products, leading to more accurate insights into best-selling products and popular discounts. Furthermore, when assessing which luxury items or discounted items were the most popular, removing outliers ensures that extreme price points (which may be errors or anomalies) don't distort our results."
      ],
      "metadata": {
        "id": "szXHriv34VbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier removal using IQR method for prices\n",
        "Q1 = data_cleaned['discount_price'].quantile(0.25)\n",
        "Q3 = data_cleaned['discount_price'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define lower and upper bounds for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Removing outliers from the discount_price column\n",
        "data_cleaned = data_cleaned[(data_cleaned['discount_price'] >= lower_bound) & (data_cleaned['discount_price'] <= upper_bound)]"
      ],
      "metadata": {
        "id": "yOQr2qWfz4Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned dataset\n",
        "cleaned_file_path = '/mnt/data/final_cleaned_dataset.csv'\n",
        "data_cleaned.to_csv(cleaned_file_path, index=False)"
      ],
      "metadata": {
        "id": "jI8L4B6Iz_P2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}